{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0cb07c-1bcc-45f4-a11a-e7aa6a2997c4",
   "metadata": {},
   "source": [
    "# Basic Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efd1430-b446-4c36-ac2f-b1f7cb765e64",
   "metadata": {},
   "source": [
    "The template for a similar implementation like last weeks's wordcount is given below.\n",
    "Adjust the code so that the word count works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d603fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wordcount.py\n",
    "\n",
    "#!/usr/bin/python3\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MyJob(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        words = line.split()\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MyJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d234c7f-ef1c-4cd8-abd8-b7221c4c65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python wordcount.py /data/dataset/text/small.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce719b-4873-4fc1-8e07-5fd19169683c",
   "metadata": {},
   "source": [
    "## Optional - Word Count with Combiners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9e295-000f-4e6b-9dc1-f4534316daa1",
   "metadata": {},
   "source": [
    "It is very easy to write combiners in MrJob. We only have to overwrite the combiner method.\n",
    "Quoted from this [blog](https://data-flair.training/blogs/hadoop-combiner-tutorial/):\n",
    "\n",
    ">On a large dataset when we run MapReduce job, large chunks of intermediate data is generated by the Mapper and this intermediate data is passed on the Reducer for further processing, which leads to enormous network congestion. MapReduce framework provides a function known as Hadoop Combiner that plays a key role in reducing network congestion.\n",
    "\n",
    "In the word-count case, we can already sum up the same words on the same machine. E.g. if we have the pair <\"cat\",1> multiple times from multiple mappers on one machine, we could sum up the number of \"cats\" and just send the total numbers of this key (\"cat\") to the reducer. This reduces network traffic. However, as we are just on a single machine, this optimization can have negative effects because we increase I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f38eae-6a5e-4289-a637-81ab7face449",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wordcountCombiner.py\n",
    "\n",
    "#!/usr/bin/python3\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MyJobCombiner(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        words = line.split()\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def combiner(self, key, values):\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MyJobCombiner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793e33c-a823-43a7-b221-adb910c75905",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python wordcountCombiner.py /data/dataset/text/small.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
