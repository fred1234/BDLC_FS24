{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0cb07c-1bcc-45f4-a11a-e7aa6a2997c4",
   "metadata": {},
   "source": [
    "# Word Count Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e02eefc-f1f0-49fb-94fe-4796441e678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wc.py\n",
    "\n",
    "#!/usr/bin/python3\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "class MyJob(MRJob):\n",
    "        \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP2 = {\n",
    "            'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options':'-nr',\n",
    "        }\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(jobconf=JOBCONF_STEP2, mapper=self.mapper_sort, reducer=self.reducer_sort)]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            yield word, 1\n",
    "            \n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, values):\n",
    "        yield word, sum(values)\n",
    "        \n",
    "    def mapper_sort(self, word, count):\n",
    "        yield int(count), word\n",
    "  \n",
    "    def reducer_sort(self, counts, words):\n",
    "        for word in words:\n",
    "            yield counts, word\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MyJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d62576-fe0e-4a67-a6f6-ec60f12536cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python wc.py /data/dataset/text/small.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea1a5e-a3db-424d-a045-da560033f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python wc.py -r hadoop hdfs:///dataset/text/small.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4a6eb-e000-40f4-9d33-f5fca7832c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python wc.py -r hadoop hdfs:///dataset/text/small.txt --output-dir hdfs:///results/wordcount/sorted/small --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf09afc-c0ff-452b-8283-10433952c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python wc.py -r hadoop hdfs:///dataset/text/holmes.txt --output-dir hdfs:///results/wordcount/sorted/holmes --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100688d-a683-4327-9bf1-e6349dc7beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took 34 minutes\n",
    "!python wc.py -r hadoop hdfs:///dataset/text/gutenberg_all.txt --output-dir hdfs:///results/wordcount/sorted/gutenberg --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370b5c8-723b-474e-a284-56156ec37a20",
   "metadata": {},
   "source": [
    "## Another Solution (no combiner, setting reduce jobs to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9432cf3-6aad-4234-a258-ac46e95ee6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wcTuned.py\n",
    "\n",
    "#!/usr/bin/python3\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "class MyJob(MRJob):\n",
    "        \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP1 = {\n",
    "            'mapreduce.job.reduces': 10\n",
    "        }\n",
    "        JOBCONF_STEP2 = {\n",
    "            'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options':'-nr',\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP1, mapper=self.mapper,      reducer=self.reducer),\n",
    "            MRStep(jobconf=JOBCONF_STEP2, mapper=self.mapper_sort, reducer=self.reducer_sort)]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            yield word, 1\n",
    "\n",
    "    def reducer(self, word, values):\n",
    "        yield word, sum(values)\n",
    "        \n",
    "    def mapper_sort(self, word, count):\n",
    "        yield int(count), word\n",
    "  \n",
    "    def reducer_sort(self, counts, words):\n",
    "        for word in words:\n",
    "            yield counts, word\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MyJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8743e81-8a40-4833-8885-62899c7f7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python wcTuned.py -r hadoop hdfs:///dataset/text/gutenberg_all.txt --output-dir hdfs:///results/wordcount/sorted/gutenberg --no-output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
