{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0cb07c-1bcc-45f4-a11a-e7aa6a2997c4",
   "metadata": {},
   "source": [
    "# Basic Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d603fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.py\n",
    "\n",
    "#!/usr/bin/python3\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MyJob(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            yield word, 1\n",
    "\n",
    "    def reducer(self, word, values):\n",
    "        yield word, sum(values)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MyJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d234c7f-ef1c-4cd8-abd8-b7221c4c65ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/wordcount.hadoop.20220308.101317.576209\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/wordcount.hadoop.20220308.101317.576209/output\n",
      "Streaming final output from /tmp/wordcount.hadoop.20220308.101317.576209/output...\n",
      "\"oder\"\t1\n",
      "\"schnelllebig\"\t1\n",
      "\"manuellen\"\t1\n",
      "\"Datenverarbeitung\"\t1\n",
      "\"und\"\t2\n",
      "\"um\"\t1\n",
      "\"umfassenden\"\t1\n",
      "\"beispielsweise\"\t1\n",
      "\"gross,\"\t1\n",
      "\"herk\\u00f6mmlichen\"\t1\n",
      "\"Data\"\t1\n",
      "\"Datafizierung\"\t1\n",
      "\"steht\"\t1\n",
      "\"strukturiert\"\t1\n",
      "\"aus\"\t1\n",
      "\"auszuwerten.\"\t1\n",
      "\"sind,\"\t1\n",
      "\"stammende\"\t1\n",
      "\"Prozess\"\t1\n",
      "\"Sprachraum\"\t1\n",
      "\"Der\"\t1\n",
      "\"Methoden\"\t1\n",
      "\"mit\"\t2\n",
      "\"bezeichnet\"\t1\n",
      "\"welche\"\t1\n",
      "\"zu\"\t4\n",
      "\"dem\"\t2\n",
      "\"Begriff\"\t1\n",
      "\"Big\"\t1\n",
      "\"Datenmengen,\"\t1\n",
      "\"Zusammenhang\"\t1\n",
      "\"der\"\t2\n",
      "\"in\"\t1\n",
      "\"komplex,\"\t1\n",
      "\"engem\"\t1\n",
      "\"englischen\"\t1\n",
      "\"schwach\"\t1\n",
      "\"sie\"\t1\n",
      "Removing temp directory /tmp/wordcount.hadoop.20220308.101317.576209...\n"
     ]
    }
   ],
   "source": [
    "!python wordcount.py /data/dataset/text/small.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce719b-4873-4fc1-8e07-5fd19169683c",
   "metadata": {},
   "source": [
    "## Optional - Word Count with Combiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73f38eae-6a5e-4289-a637-81ab7face449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcountCombiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcountCombiner.py\n",
    "\n",
    "#!/usr/bin/python3\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MyJobCombiner(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            yield word, 1\n",
    "        \n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MyJobCombiner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1793e33c-a823-43a7-b221-adb910c75905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/wordcountCombiner.hadoop.20220308.101321.111933\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/wordcountCombiner.hadoop.20220308.101321.111933/output\n",
      "Streaming final output from /tmp/wordcountCombiner.hadoop.20220308.101321.111933/output...\n",
      "\"manuellen\"\t1\n",
      "\"herk\\u00f6mmlichen\"\t1\n",
      "\"Datenmengen,\"\t1\n",
      "\"stammende\"\t1\n",
      "\"sie\"\t1\n",
      "\"sind,\"\t1\n",
      "\"Zusammenhang\"\t1\n",
      "\"engem\"\t1\n",
      "\"englischen\"\t1\n",
      "\"Big\"\t1\n",
      "\"Data\"\t1\n",
      "\"schwach\"\t1\n",
      "\"Sprachraum\"\t1\n",
      "\"schnelllebig\"\t1\n",
      "\"Der\"\t1\n",
      "\"Methoden\"\t1\n",
      "\"Datenverarbeitung\"\t1\n",
      "\"in\"\t1\n",
      "\"komplex,\"\t1\n",
      "\"aus\"\t1\n",
      "\"auszuwerten.\"\t1\n",
      "\"um\"\t1\n",
      "\"umfassenden\"\t1\n",
      "\"und\"\t2\n",
      "\"welche\"\t1\n",
      "\"steht\"\t1\n",
      "\"strukturiert\"\t1\n",
      "\"beispielsweise\"\t1\n",
      "\"Begriff\"\t1\n",
      "\"Datafizierung\"\t1\n",
      "\"zu\"\t4\n",
      "\"Prozess\"\t1\n",
      "\"bezeichnet\"\t1\n",
      "\"gross,\"\t1\n",
      "\"dem\"\t2\n",
      "\"der\"\t2\n",
      "\"mit\"\t2\n",
      "\"oder\"\t1\n",
      "Removing temp directory /tmp/wordcountCombiner.hadoop.20220308.101321.111933...\n"
     ]
    }
   ],
   "source": [
    "!python wordcountCombiner.py /data/dataset/text/small.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
