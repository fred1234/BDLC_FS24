{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed91e618",
      "metadata": {},
      "source": [
        "# Map Reduce"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0d8f87e",
      "metadata": {},
      "source": [
        "## Write the Mapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f0a05673-16d5-4adb-8312-fcffdb609179",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/python3\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    words = line.split()\n",
        "    for word in words:\n",
        "        print(f\"{word}\\t1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "863428a2",
      "metadata": {},
      "source": [
        "## Write the Reducer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d320d856",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile reducer.py\n",
        "#!/usr/bin/python3\n",
        "\n",
        "import sys\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    try:\n",
        "        # convert count (currently a string) to int\n",
        "        word, count = line.split('\\t', 1)\n",
        "        count = int(count)\n",
        "    except ValueError:\n",
        "        # split was not right or count was not a number, so silently\n",
        "        # ignore/discard this line\n",
        "        continue\n",
        "    if current_word == word:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_word:\n",
        "            print(f\"{current_word}\\t{current_count}\")\n",
        "        current_count = count\n",
        "        current_word = word\n",
        "\n",
        "if current_word == word:\n",
        "    print(f\"{current_word}\\t{current_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f082b6f1-826b-4d02-b73f-e42db9efc713",
      "metadata": {},
      "source": [
        "## Permission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bc55cc25-995a-4c0d-a521-141f7459a378",
      "metadata": {},
      "outputs": [],
      "source": [
        "!chmod 755 *.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "897d4b24-b5b1-40f7-971b-2627b5a113e6",
      "metadata": {},
      "source": [
        "## Word Count with holmes.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3986e9fc-00f4-4c07-897d-5416c6e59d82",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/hadoop/BDLC_FS23/V03/resources/wordcount_map_reduce\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b09d2dbd-7b13-4080-8a25-3891d03688d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6 items\n",
            "drwxr-xr-x   - hadoop supergroup          0 2022-03-06 13:14 /dataset\n",
            "drwxr-xr-x   - hadoop supergroup          0 2022-03-07 12:08 /own_word_count\n",
            "drwxr-xr-x   - hadoop supergroup          0 2022-03-02 13:47 /own_word_count_small_file\n",
            "drwxr-xr-x   - hadoop supergroup          0 2022-03-02 10:38 /test\n",
            "drwxrwx---   - hadoop supergroup          0 2022-03-02 07:41 /tmp\n",
            "drwxr-xr-x   - hadoop supergroup          0 2022-03-02 07:39 /user\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls /"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "20d20708-dca3-4833-a6d0-34ed9e3d8e9a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted /own_word_count\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -rm -r /own_word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09cb269b",
      "metadata": {},
      "outputs": [],
      "source": [
        "!hadoop jar ~/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar \\\n",
        "-files /home/hadoop/BDLC_FS23/V03/resources/wordcount_map_reduce/ \\\n",
        "-mapper /home/hadoop/BDLC_FS23/V03/resources/wordcount_map_reduce/mapper.py \\\n",
        "-reducer /home/hadoop/BDLC_FS23/V03/resources/wordcount_map_reduce/reducer.py \\\n",
        "-input /dataset/text/holmes.txt \\\n",
        "-output /own_word_count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef46384-9a58-4fb7-9f2e-bb1d1a3efbed",
      "metadata": {},
      "source": [
        "## Word Count with gutenberg_all.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "87267ce5-0b91-4a8d-ab97-2ac3899768b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 items\n",
            "-rw-r--r--   1 hadoop supergroup 10823456892 2022-03-06 13:15 /dataset/text/gutenberg_all.txt\n",
            "-rw-r--r--   1 hadoop supergroup      607430 2022-03-06 13:14 /dataset/text/holmes.txt\n",
            "-rw-r--r--   1 hadoop supergroup         342 2022-03-06 13:14 /dataset/text/small.txt\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls /dataset/text/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "20d357bd-1bc9-4e85-84ea-38d46b14791b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted /own_word_count\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -rm -r /own_word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "39f27e27-1b5c-4fab-bd02-931afc1ea578",
      "metadata": {},
      "outputs": [],
      "source": [
        "# see htop and df -h \n",
        "# while true; do df -h | grep -e \"Filesystem\\|/$\\|/data\"; sleep 1; clear; done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fccfbcfe-f3ae-4394-88d0-1999e328759d",
      "metadata": {},
      "outputs": [],
      "source": [
        "!hadoop jar ~/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar \\\n",
        "-files /home/hadoop/BDLC_FS23/V03/resources/wordcount_map_reduce/ \\\n",
        "-mapper /home/hadoop/BDLC_FS23/V03/resources/wordcount_map_reduce/mapper.py \\\n",
        "-reducer /home/hadoop/BDLC_FS23/V03/resources/wordcount_map_reduce/reducer.py \\\n",
        "-input /dataset/text/gutenberg_all.txt \\\n",
        "-output /own_word_count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1429c01-6d9a-4e08-9a65-e68709a759a7",
      "metadata": {},
      "source": [
        "## Word Count with gutenberg_all.txt (Tuned Cluster) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be58268-d405-4686-9958-173b79c4c240",
      "metadata": {},
      "source": [
        "Let us use 10 reducers for parallelism. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a7d703ca-d039-4015-8054-190521cfb202",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted /own_word_count\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -rm -r /own_word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34a026b0-ac40-43b4-8fda-ebd79ae10896",
      "metadata": {},
      "outputs": [],
      "source": [
        "!hadoop jar ~/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar \\\n",
        "-files /home/hadoop/BDLC_FS23/V03/resources/wordcount_map_reduce/ \\\n",
        "-mapper /home/hadoop/BDLC_FS23/V03/resources/wordcount_map_reduce/mapper.py \\\n",
        "-reducer /home/hadoop/BDLC_FS23/V03/resources/wordcount_map_reduce/reducer.py \\\n",
        "-input /dataset/text/gutenberg_all.txt \\\n",
        "-output /own_word_count \\\n",
        "-numReduceTasks 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f2896f-f006-4662-a686-6dc56a61ba54",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
