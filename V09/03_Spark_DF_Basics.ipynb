{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9171ad56-d901-4e60-ba32-b5b6dba59cf0",
      "metadata": {},
      "source": [
        "# Init Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021061aa-a851-4386-9dc1-de52b8a8ba60",
      "metadata": {},
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Spark Basics\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13033a30-108b-4a0a-adef-34ad6f8828c6",
      "metadata": {},
      "source": [
        "# Prepare Data\n",
        "\n",
        "This content is from the book [Spark-The-Definitive-Guide](https://github.com/databricks/Spark-The-Definitive-Guide)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ee8f1f-2106-44fd-8d89-c4a34414a2f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://github.com/databricks/Spark-The-Definitive-Guide/archive/refs/heads/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33953e5e-4de8-41ce-9652-75ef2affbe6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "!mv master.zip book.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f486b031-dc51-48f6-935c-df79450a8b6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip -qq book.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf3e5dd8-3aa1-4c94-9f3f-d1a86d434eb2",
      "metadata": {},
      "source": [
        "## Path Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e8700f9-1228-4f4b-ab93-be517554eeb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "path = os.getcwd()\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2eb7c9a-a54b-459b-bda6-e202f8d5a1ec",
      "metadata": {},
      "source": [
        "## A Gentle Introduction to Spark / Chapter 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbb6903c-22a3-4fab-8ed1-efc9dbd817c7",
      "metadata": {},
      "source": [
        "### Creating DFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb0075b7-7797-4e03-8046-e15c696c54ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "myRange = spark.range(1000).toDF(\"number\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ee2ff7-4bb7-4f3e-874b-365918420b17",
      "metadata": {},
      "outputs": [],
      "source": [
        "divisBy2 = myRange.where(\"number % 2 = 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e798fbb5-5062-45e2-be6a-81da5936e30d",
      "metadata": {},
      "outputs": [],
      "source": [
        "flightData2015 = spark\\\n",
        "  .read\\\n",
        "  .option(\"inferSchema\", \"true\")\\\n",
        "  .option(\"header\", \"true\")\\\n",
        "  .csv(f\"file:///{path}/Spark-The-Definitive-Guide-master/data/flight-data/csv/2015-summary.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68618881-3b85-45b4-8764-076761dcc21f",
      "metadata": {},
      "source": [
        "### SQL vs DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6470c06-4b00-406f-9f93-807101075161",
      "metadata": {},
      "outputs": [],
      "source": [
        "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a856b3f6-031f-4779-b55a-c345b557fe31",
      "metadata": {},
      "outputs": [],
      "source": [
        "sqlWay = spark.sql(\"\"\"\n",
        "SELECT DEST_COUNTRY_NAME, count(1)\n",
        "FROM flight_data_2015\n",
        "GROUP BY DEST_COUNTRY_NAME\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef75b191-5950-424d-8d75-ed9de6b08b92",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataFrameWay = flightData2015\\\n",
        "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        "  .count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1dd79c1-2686-444e-bd2f-1723fc09588c",
      "metadata": {},
      "outputs": [],
      "source": [
        "sqlWay.explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86938e79-c4c9-4c18-bb5a-5541bf246944",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataFrameWay.explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77830159-9649-4a70-bb67-aa52c933f988",
      "metadata": {},
      "source": [
        "### Aggregates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9d0246b-3ed6-4206-b5a6-faafdc89396b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import max\n",
        "# https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.functions.max.html\n",
        "\n",
        "# SELECT max(count) FROM flight_data_2015 LIMIT 1\n",
        "flightData2015.select(max(\"count\")).take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f7d9e1-13a2-4d79-af58-8a14c8a50190",
      "metadata": {},
      "outputs": [],
      "source": [
        "maxSql = spark.sql(\"\"\"\n",
        "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
        "FROM flight_data_2015\n",
        "GROUP BY DEST_COUNTRY_NAME\n",
        "ORDER BY sum(count) DESC\n",
        "LIMIT 5\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "185e4d25-29cf-4aee-834f-7478e9301277",
      "metadata": {},
      "outputs": [],
      "source": [
        "maxSql.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9c607b2-b882-4859-8fb8-acf7909b9fab",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "flightData2015\\\n",
        "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        "  .sum(\"count\")\\\n",
        "  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
        "  .sort(desc(\"destination_total\"))\\\n",
        "  .limit(5)\\\n",
        "  .show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0313ce8c-0f35-480e-b6e1-8795f5384a19",
      "metadata": {},
      "source": [
        "## Structured API Overview/ Chapter 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b695a35-aa9f-46f7-a7ad-131020aa68ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.range(500).toDF(\"number\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9cb499-dfac-48e2-b5b6-1a36861a97d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(df[\"number\"] + 10).show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f6bfcfc-5065-43b8-bbb3-dcec969488ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(df.number + 10).show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dafc6056-d55c-42f1-b120-b18249f405d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "df.select(col(\"number\") + 10).show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a108037d-31df-472b-82a6-796873c3b21f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "df.select(expr(\"number + 10\")).show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d032a92f-e4f1-42bd-8246-aac3eeb542ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.selectExpr(\"number + 10\").show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a17dc0-e354-4f52-bb98-5990fdbb3742",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.limit(4).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdf0ad78-6eba-4772-866d-8ac44fb303b2",
      "metadata": {},
      "source": [
        "## Basic Structured Operations / Chapter 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6252dd9c-8fc5-40a7-9081-ed12fcf87a18",
      "metadata": {},
      "source": [
        "### Schemas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3754a3f8-fc3f-4fc3-bc74-26795d1f8f0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.read.format(\"json\").\\\n",
        "    load(f\"file:///{path}/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a59f304c-6884-40db-a7e5-3b574a96a7e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64967d9b-638d-4392-a826-74f68633ac93",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf04b5e-c3f8-47ea-87a6-5f494c4834db",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "\n",
        "myManualSchema = StructType([\n",
        "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
        "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
        "  StructField(\"count\", LongType(), False)\n",
        "])\n",
        "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
        "  .load(f\"file:///{path}/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb363fb-8da5-46b0-a309-9ad59f91e640",
      "metadata": {},
      "source": [
        "### Columns and Expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca6931db-4e70-49e5-807f-7a2527e544fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, column\n",
        "print(col(\"someColumnName\"))\n",
        "print(column(\"someColumnName\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dceae00-0e2b-44d7-ae9c-ddac48c17be0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b189f03-0ab0-444d-beed-57b34c4f6c8a",
      "metadata": {},
      "source": [
        "### Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb43fc5-e7c5-4f6a-8c67-e7217395fd5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "myRow = Row(\"Hello\", None, 1, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2829eb8-3fec-49af-bc84-f76193cf28f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(myRow[0])\n",
        "print(myRow[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "174087d5-b1cd-4e63-a760-3913190cf2ea",
      "metadata": {},
      "source": [
        "### DataFrame Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea67979-50ac-404c-a1b7-de148beb22f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.read.format(\"json\").\\\n",
        "    load(f\"file:///{path}/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
        "df.createOrReplaceTempView(\"dfTable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b790dd52-4b45-48ce-892d-2eb6a897b034",
      "metadata": {},
      "source": [
        "#### Select and SelectExpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0484cbc5-9ddd-40f9-ab3e-093eda1a52ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(\"DEST_COUNTRY_NAME\").show(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f30e9eb-4f8a-441d-a7d6-b1cd91cce758",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b2c3be-4741-467e-be2a-f6b890a744a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr, col, column\n",
        "df.select(\n",
        "    expr(\"DEST_COUNTRY_NAME\"),\n",
        "    col(\"DEST_COUNTRY_NAME\"),\n",
        "    column(\"DEST_COUNTRY_NAME\"))\\\n",
        "  .show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ca5798-3535-4115-bc59-12e7da71f675",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2b2ffe-5fad-4fa0-853e-c63d603fc521",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\"))\\\n",
        "  .show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3319b76c-dc11-4fcd-b472-12172f00c661",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6dc7399-b8da-47b5-bd95-af589f1524e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.selectExpr(\n",
        "  \"*\", # all original columns\n",
        "  \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
        "  .show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b27ebcf-67dc-4eef-8d9b-6f9b61469857",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb06b88-69a1-4ce9-a34b-47ed5c78f27f",
      "metadata": {},
      "source": [
        "#### Literals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "907f098e-fe6b-4372-89e7-e4f9ef557d93",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9cd8e4-0064-4c1b-b2b4-00b58b81b53f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.selectExpr(\"*\", \"1 as One\").show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919c1c36-0ab9-46e5-a4ee-f8c2297efa98",
      "metadata": {},
      "source": [
        "#### Adding Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3bf28c-3bb0-4c6c-9571-7ec7fbc15baf",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.withColumn(\"numberOne\", lit(1)).show(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b32c65b0-970a-4080-94f8-c301ef59d380",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n",
        "  .show(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8c93713-f76f-40e8-b59d-ca9a753b6ef9",
      "metadata": {},
      "source": [
        "#### Renaming Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e181c9-ff21-4b38-adb8-5ce9bbeb14a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39d1e33f-aec7-44df-a065-a06301f3768f",
      "metadata": {},
      "outputs": [],
      "source": [
        "dfWithLongColName = df.withColumn(\n",
        "    \"This Long Column-Name\",\n",
        "    expr(\"ORIGIN_COUNTRY_NAME\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d01dbea-94d4-45d3-aead-ad986141cd90",
      "metadata": {},
      "outputs": [],
      "source": [
        "dfWithLongColName.selectExpr(\n",
        "    \"`This Long Column-Name`\",\n",
        "    \"`This Long Column-Name` as `new col`\")\\\n",
        "  .show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d5ccd9-3407-4fd3-a3c5-7266f9cbf0be",
      "metadata": {},
      "outputs": [],
      "source": [
        "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1115e3ed-d8fc-480d-804b-047f2e0d2d4b",
      "metadata": {},
      "source": [
        "#### Dropping Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6867971c-0504-44c0-b225-8cbaa6b1c135",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cc29207-f1a2-45d3-b24c-362b0d244450",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.drop(\"ORIGIN_COUNTRY_NAME\").show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57fc2c9d-5453-4522-aff6-6475c04e0131",
      "metadata": {},
      "source": [
        "#### Filtering Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86d507c-3cd6-4d99-b5d4-2f92ac302683",
      "metadata": {},
      "outputs": [],
      "source": [
        "# filter == where\n",
        "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n",
        "  .show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fea3621-6c9a-43bc-a00a-db8f386ce568",
      "metadata": {},
      "source": [
        "#### Unique Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8fb2ffb-c3b1-4d3a-bffd-2b0d4bc16f7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52933cac-f475-4d2f-9e7f-23b2b84ff775",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12bfec08-9eb5-414c-825e-5eaeb64afc7d",
      "metadata": {},
      "source": [
        "#### Random Samples / Random Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fccc3cb-e3f6-45f1-a919-59b7eb576c11",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58fd5c1a-e7fa-4780-a901-d29fe545440f",
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 5\n",
        "withReplacement = False\n",
        "fraction = 0.5\n",
        "df.sample(withReplacement, fraction, seed).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf99f720-c9c0-4edb-9325-9c75764a737c",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
        "dataFrames[0].count() > dataFrames[1].count() # False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db2c2812-cec1-4626-8073-02d1bb45dfd2",
      "metadata": {},
      "source": [
        "#### Concatenating and Appending Rows (Union)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f55dc1c-c528-49d5-a6cd-d58a84f3b037",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "schema = df.schema\n",
        "newRows = [\n",
        "  Row(\"New Country\", \"Other Country\", 5),\n",
        "  Row(\"New Country 2\", \"Other Country 3\", 1)\n",
        "]\n",
        "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
        "newDF = spark.createDataFrame(parallelizedRows, schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a9937e-bf8e-4dd7-8062-a79f0ac1afa2",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.union(newDF)\\\n",
        "  .where(\"count = 1\")\\\n",
        "  .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd36791-6496-4c57-aef6-bbdc7dbaa09d",
      "metadata": {},
      "source": [
        "#### Sorting Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b41358f-bd0e-4963-b93e-cd7682e18b60",
      "metadata": {},
      "outputs": [],
      "source": [
        "# sort === orderBy\n",
        "df.sort(\"count\").show(5)\n",
        "df.(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
        "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39bc4f30-9704-4020-bbe1-12896ffc2bd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import desc, asc\n",
        "df.orderBy(expr(\"count desc\")).show(2)\n",
        "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fc2cd4d-72b2-4086-a1d9-4b89e1e84843",
      "metadata": {},
      "source": [
        "#### Limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0406f33-6af5-45bd-83e5-2c148a03d425",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.orderBy(expr(\"count desc\")).limit(2).show(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70649fab-1143-4da4-90c7-a1c86214b04f",
      "metadata": {},
      "source": [
        "#### Repartition "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8af2e5f8-8ad0-42df-98cc-14b772cd9b43",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.rdd.getNumPartitions() # 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2150d5-a74d-44cf-ac1b-05a34e8dabef",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.repartition(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "103ab969-76eb-4fbb-9c16-9ec81bcf26b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.rdd.getNumPartitions() # 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b0c17a-7281-4cf3-9c2d-2417690334af",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65987ca8-e7eb-4456-91a1-eccd1a7f892c",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4679cf-9dd3-449a-ba3b-b22613ad96dd",
      "metadata": {},
      "source": [
        "#### Collecting Rows To Driver "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a29df0f8-4675-499a-982d-24e36adbd3ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "collectDF = df.limit(10)\n",
        "collectDF.take(5) # take works with an Integer count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd57ac42-2138-4fe9-a598-45107e73b5df",
      "metadata": {},
      "outputs": [],
      "source": [
        "collectDF.show() # this prints it out nicely\n",
        "collectDF.show(5, False)\n",
        "collectDF.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f3e5a2-3511-42e3-8813-e74aff05c7d1",
      "metadata": {},
      "source": [
        "## Working with Different Types of Data / Chapter 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aad256ae-2604-40b3-b488-1623569703dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.read.format(\"csv\")\\\n",
        "  .option(\"header\", \"true\")\\\n",
        "  .option(\"inferSchema\", \"true\")\\\n",
        "  .load(f\"file:///{path}/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2010-12-01.csv\")\n",
        "df.printSchema()\n",
        "df.createOrReplaceTempView(\"dfTable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0739dda4-4ef2-4750-860c-a24d2f7be6e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "df.select(lit(5), lit(\"five\"), lit(5.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce8dea8-7a43-4b3c-b66d-80f5ff28b3ee",
      "metadata": {},
      "source": [
        "### Working with Booleans "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78b2317d-ba11-4e7c-8346-dc4c4e60892e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "df.where(col(\"InvoiceNo\") != 536365)\\\n",
        "  .select(\"InvoiceNo\", \"Description\")\\\n",
        "  .show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac86f95-8d7a-4cf0-9bb7-1b6ae6f18193",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.where(\"InvoiceNo <> 536365\")\\\n",
        "  .select(\"InvoiceNo\", \"Description\")\\\n",
        "  .show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36220933-1d8a-41c5-a77b-294b9aabd6e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import instr\n",
        "priceFilter = col(\"UnitPrice\") > 600\n",
        "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
        "\n",
        "df.where(\"StockCode = 'DOT'\").where(priceFilter | descripFilter).show(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a1b483a-05d4-412a-bbe7-12c7a06e3e01",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import instr\n",
        "\n",
        "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
        "priceFilter = col(\"UnitPrice\") > 600\n",
        "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
        "\n",
        "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",
        "  .where(\"isExpensive\")\\\n",
        "  .select(\"unitPrice\", \"isExpensive\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46db78d0-35cf-4d77-94b9-8a26a18fa9bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\\\n",
        "  .where(\"isExpensive\")\\\n",
        "  .select(\"Description\", \"UnitPrice\").show(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0b91f2-924f-46c4-b349-25e300963363",
      "metadata": {},
      "source": [
        "### Working with Numbers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20fd773f-c1dc-4d74-bda2-063497d18935",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr, pow\n",
        "\n",
        "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
        "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9ff01e-037b-48d1-a713-5c0b49a33bf3",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.selectExpr(\n",
        "  \"CustomerId\",\n",
        "  \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1d235d-b79b-4e9f-bf12-5ecdedda2ece",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lit, round, bround\n",
        "\n",
        "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9518dcb3-0d4d-408e-8b12-c15e6b1e2d18",
      "metadata": {},
      "source": [
        "### Stats "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bff442d-e34f-47c3-a607-ffded021196c",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.stat.corr(\"Quantity\", \"UnitPrice\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "626936bc-f65c-49f5-8c0b-638cf68e3806",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import corr\n",
        "df.select(corr(\"Quantity\", \"UnitPrice\")).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3f14155-25c6-4d10-b3f6-65f69866e1f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de450a0-0b20-4f2e-8b55-7cf0f0cba276",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count, mean, stddev_pop, min, max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12fc6646-d4f6-4b42-8a69-88f8e285566d",
      "metadata": {},
      "outputs": [],
      "source": [
        "colName = \"UnitPrice\"\n",
        "quantileProbs = [0.5]\n",
        "relError = 0.05\n",
        "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError) # 2.51"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bea5c6d-e391-4f18-8069-a26d4e6571de",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.stat.crosstab(\"StockCode\", \"Quantity\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f92b72-9ec2-4c75-ac3c-c9208ccbe16b",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.stat.freqItems([\"StockCode\", \"Quantity\"]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5151ef51-e296-40f7-acda-41f236ad8f16",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "df.select(monotonically_increasing_id()).show(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a934169-c831-4db7-afec-b404c9820497",
      "metadata": {},
      "source": [
        "### Working with Strings "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7257c7f8-4b10-4c8f-a220-e1beacb019ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import initcap\n",
        "df.select(initcap(col(\"Description\"))).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dbda296-6c9d-4f27-87be-b8fcbb5eabea",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lower, upper\n",
        "df.select(col(\"Description\"),\n",
        "    lower(col(\"Description\")),\n",
        "    upper(lower(col(\"Description\")))).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193dcfd3-d203-48ac-a06b-0352c742b0fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
        "df.select(\n",
        "    ltrim(lit(\"    HELLO    \")).alias(\"ltrim\"),\n",
        "    rtrim(lit(\"    HELLO    \")).alias(\"rtrim\"),\n",
        "    trim(lit(\"    HELLO    \")).alias(\"trim\"),\n",
        "    lpad(lit(\"HELLO\"), 10, \" \").alias(\"lp\"),\n",
        "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad805737-6680-4fea-b0a6-73e78b1614f4",
      "metadata": {},
      "source": [
        "### Regexp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54bcc52d-1e23-4deb-b655-7fdddded46ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n",
        "df.select(\n",
        "  regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n",
        "  col(\"Description\")).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5af17e-f776-4ec2-91d6-4cc1ce0aa037",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import translate\n",
        "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"),col(\"Description\"))\\\n",
        "  .show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ac133c-f4cc-41c0-932a-721b148ce87c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
        "df.select(\n",
        "     regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n",
        "     col(\"Description\")).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8804e574-6f28-46d5-bec0-87f22910e1c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import instr\n",
        "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
        "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
        "\n",
        "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
        "  .where(\"hasSimpleColor\")\\\n",
        "  .select(\"Description\").show(3, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "223047e4-010b-4764-b3ca-74e9382617f4",
      "metadata": {},
      "source": [
        "### Dates and Timestamps "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0cfd078-b8a6-4abc-bce5-ea671b45e3f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import current_date, current_timestamp\n",
        "\n",
        "dateDF = spark.range(10)\\\n",
        "  .withColumn(\"today\", current_date())\\\n",
        "  .withColumn(\"now\", current_timestamp())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d51bd5-47fd-4295-94f9-75484bb9f957",
      "metadata": {},
      "outputs": [],
      "source": [
        "dateDF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cd774a3-3f24-48f1-aefa-62c4ecbbef0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import date_add, date_sub\n",
        "\n",
        "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1cd409-f381-4a03-8081-df2d7ddb8b84",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import datediff, months_between, to_date\n",
        "\n",
        "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
        "  .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c998f0cd-eeb9-445b-ac65-faeb2d4760c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "dateDF.select(\n",
        "    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
        "    to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
        "  .select(months_between(col(\"start\"), col(\"end\"))).show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5541841-1b66-47e2-9728-7a5829680bcd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_date, lit\n",
        "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",
        "  .select(to_date(col(\"date\"))).show(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d39ed9-9fdc-4cb8-a32a-552f0f92ece6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "dateFormat = \"yyyy-dd-MM\"\n",
        "cleanDateDF = spark.range(1).select(\n",
        "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
        "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "755ebbaf-b8dc-4369-a8ea-d9ca77f2a720",
      "metadata": {},
      "outputs": [],
      "source": [
        "cleanDateDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25573d3e-e5bd-44c4-b6dd-694335275201",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_timestamp\n",
        "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e28117-c9ee-43b2-8115-8f5db3020e14",
      "metadata": {},
      "source": [
        "### Working with Nulls in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d31980c-4988-4353-9a31-fc0e623ce118",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9c8042-93ab-4722-b412-108533f77049",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b54acc4c-f218-400c-b606-0347e4a94ad4",
      "metadata": {},
      "outputs": [],
      "source": [
        "fill_cols_vals = {\"StockCode\": 5, \"Description\" : \"No Value\"}\n",
        "df.na.fill(fill_cols_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e559321c-e910-4658-9e6f-e56d980e4448",
      "metadata": {},
      "source": [
        "### Working with Complex Types "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd396d22-2ae6-4fc7-93da-f686166289c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import struct\n",
        "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3adaee7c-8ff9-4374-95bc-a36823368758",
      "metadata": {},
      "outputs": [],
      "source": [
        "complexDF.show(2, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bc53f9-2c55-4c12-bb24-af54cb61bc5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "complexDF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d81f75-088b-4313-959c-3c1b7e6a0031",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import split\n",
        "df.select(split(col(\"Description\"), \" \")).show(2, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cbfbb7d-ad12-4d7e-ad55-520ca5015140",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n",
        "  .selectExpr(\"array_col[0]\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39d8057d-0152-4669-89d0-f31a534fc2b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import size\n",
        "df.select(size(split(col(\"Description\"), \" \"))).show(2) # shows 5 and 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67baf2bf-acec-4bf4-8398-eaf26dbe9e66",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import array_contains\n",
        "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "826ddc64-b591-48c6-ace0-f54b2cac2d06",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import split, explode\n",
        "\n",
        "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
        "  .withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
        "  .select(\"Description\", \"InvoiceNo\", \"exploded\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38484d04-949e-4ca1-9676-5c6b42f9b6a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import create_map\n",
        "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
        "  .show(2,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60507be8-f49a-414f-825d-3d365d34c7b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
        "  .selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c5cb2c-2e32-4b02-9856-aa4c21d34830",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
        "  .selectExpr(\"explode(complex_map)\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ae2ca8-80ef-4f63-a5d0-89f6daa1d747",
      "metadata": {},
      "outputs": [],
      "source": [
        "jsonDF = spark.range(1).selectExpr(\"\"\"\n",
        "  '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ea2369-a0e7-4403-b8b0-a78fccf7909c",
      "metadata": {},
      "outputs": [],
      "source": [
        "jsonDF.show(1, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c37b681-3a05-4798-97c1-297c15dfb861",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import get_json_object, json_tuple\n",
        "\n",
        "jsonDF.select(\n",
        "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
        "    json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d9c026-1b67-4106-809a-76f094e3ca37",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_json\n",
        "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
        "  .select(to_json(col(\"myStruct\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ea8158-c387-4ff0-9001-213609c93b65",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
        "  .select(to_json(col(\"myStruct\"))).show(20,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c7844b5-1ea0-4863-9a35-082525200a5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import from_json\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "parseSchema = StructType((\n",
        "  StructField(\"InvoiceNo\",StringType(),True),\n",
        "  StructField(\"Description\",StringType(),True)))\n",
        "\n",
        "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
        "  .select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
        "  .select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2, False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37636205-3f56-4dea-82ae-0bd685c996ea",
      "metadata": {},
      "source": [
        "## Aggregations / Chapter 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f7b553-2ef1-4999-8e73-104ef48197c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.read.format(\"csv\")\\\n",
        "  .option(\"header\", \"true\")\\\n",
        "  .option(\"inferSchema\", \"true\")\\\n",
        "  .load(f\"file:///{path}/Spark-The-Definitive-Guide-master/data/retail-data/all/*.csv\")\\\n",
        "  .coalesce(5)\n",
        "df.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8149c607-ddf9-422f-8ca1-acca9a5fcb38",
      "metadata": {},
      "source": [
        "### Counting "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70922255-9158-478d-a019-23b1abb8bb15",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count\n",
        "df.select(count(\"StockCode\")).show() # 541909"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48fb6061-01db-4740-b3ba-cbd9ee9d1f66",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "df.select(countDistinct(\"StockCode\")).show() # 4070"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84428bf3-fbde-41ad-93fc-ab320930a37d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import approx_count_distinct\n",
        "df.select(approx_count_distinct(\"StockCode\", 0.1)).show() # 3364\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c41f35bf-23a8-488e-baf3-1fd0eb99638f",
      "metadata": {},
      "source": [
        "### First and Last "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407db3a1-d714-4ef0-92aa-b5a6a63a22f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import first, last\n",
        "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38f1149d-13e1-4a86-9381-a1c106ac62b1",
      "metadata": {},
      "source": [
        "#### Min and Max "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "813434a0-bdc3-4232-864d-1acf52257f48",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import min, max\n",
        "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "636856e4-a8e4-4ffa-bba9-ca1e1f209a1c",
      "metadata": {},
      "source": [
        "### Sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c878bc3-b14f-49e3-8e72-c35599705b2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import sum\n",
        "df.select(sum(\"Quantity\")).show() # 5176450"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a045673-384b-4c91-9bfb-1fc94682df28",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import sum_distinct\n",
        "df.select(sum_distinct(\"Quantity\")).show() # 29310"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c9013b7-1228-41a5-a574-574040de52ba",
      "metadata": {},
      "source": [
        "### Avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf0807e3-de58-441d-bd00-dd9ba14513bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import sum, count, avg, expr\n",
        "\n",
        "df.select(\n",
        "    count(\"Quantity\").alias(\"total_transactions\"),\n",
        "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
        "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
        "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
        "  .selectExpr(\n",
        "    \"total_purchases/total_transactions\",\n",
        "    \"avg_purchases\",\n",
        "    \"mean_purchases\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80646fec-2465-456b-b536-74bfcf52b234",
      "metadata": {},
      "source": [
        "### Variance / STD / Skewness etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d47dd1b-56dd-4564-994e-d4dae0c4bc6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import var_pop, stddev_pop\n",
        "from pyspark.sql.functions import var_samp, stddev_samp\n",
        "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
        "  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db2b0b2-a905-4075-afe6-28bd4c27e13f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import skewness, kurtosis\n",
        "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b197d4-74a6-4b05-8703-40fade43f6f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
        "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
        "    covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac56ca5b-44c7-4917-9a10-b9d79b8d7b15",
      "metadata": {},
      "source": [
        "### Aggregating to Complex Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5cb59f-1bf9-4c5b-b666-eeb2792b6799",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import collect_set, collect_list\n",
        "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008c2375-1912-47d8-ad0d-31eea0b8802e",
      "metadata": {},
      "source": [
        "### Grouping "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ff396b-8d9a-4716-90c1-877731d19570",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "df.groupBy(\"InvoiceNo\").agg(\n",
        "    count(\"Quantity\").alias(\"quan\"),\n",
        "    expr(\"count(Quantity)\")).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f2330b5-d6e6-4330-b451-38b935f16d1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227c80a1-c6f8-41e4-a7c2-5a622b78ef2c",
      "metadata": {},
      "source": [
        "## Joins / Chapter 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67988783-8ec3-4693-8d5d-9ed36bb3f003",
      "metadata": {},
      "outputs": [],
      "source": [
        "person = spark.createDataFrame([\n",
        "    (0, \"Bill Chambers\", 0, [100]),\n",
        "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
        "    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
        "  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
        "\n",
        "graduateProgram = spark.createDataFrame([\n",
        "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
        "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
        "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
        "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
        "\n",
        "sparkStatus = spark.createDataFrame([\n",
        "    (500, \"Vice President\"),\n",
        "    (250, \"PMC Member\"),\n",
        "    (100, \"Contributor\")])\\\n",
        "  .toDF(\"id\", \"status\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae7c1171-49ce-43cf-8400-e8ecda68e46f",
      "metadata": {},
      "outputs": [],
      "source": [
        "person.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f759569b-4864-4760-a1bf-45bb4f138732",
      "metadata": {},
      "outputs": [],
      "source": [
        "graduateProgram.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b2d827-8629-478c-bb4f-039f9ad19b8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "sparkStatus.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adb379bd-6ddc-4286-84fb-899394116c8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "person.join(graduateProgram, person[\"graduate_program\"] == graduateProgram['id']).show(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "178379c4-e2d3-4c4f-bd69-2553be241b5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "person.join(graduateProgram, person[\"graduate_program\"] == graduateProgram['id'], \"right_outer\").show(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12cfc6ed-25c4-4b2b-b75c-c85dbab116e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "person.withColumnRenamed(\"id\", \"personId\")\\\n",
        "  .join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
